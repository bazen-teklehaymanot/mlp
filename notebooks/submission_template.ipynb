{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ… Submission\n",
    "\n",
    "This section will contain a reusable utility function to submit the predictions to the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q pandas numpy scikit-learn xgboost imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_DIR = '/kaggle/input/costa-rican-household-poverty-prediction/'\n",
    "DATA_DIR = '../data/'\n",
    "\n",
    "TEST_CSV = DATA_DIR + 'test.csv'\n",
    "TRAIN_CSV = DATA_DIR + 'train.csv'\n",
    "TEST_CSV = DATA_DIR + 'test.csv'\n",
    "\n",
    "TARGET_COLUMN = \"Target\"\n",
    "ID_COLUMN = \"Id\"\n",
    "HOUSE_HOLD_ID_COLUMN = \"idhogar\"\n",
    "\n",
    "DEFAULT_RANDOM_STATE = 369\n",
    "DEFAULT_TEST_SIZE = 0.4\n",
    "DEFAULT_VALIDATION_SIZE = 0.4\n",
    "DEFAULT_CROSS_VALIDATION = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from copy import deepcopy\n",
    "\n",
    "pipeline = None\n",
    "\n",
    "def fill_and_encode(data, fit=False):\n",
    "    global pipeline\n",
    "    num_cols = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "    cat_cols = data.select_dtypes(include=['object', 'bool']).columns  \n",
    "\n",
    "    num_transformer = SimpleImputer(strategy='median') \n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', num_transformer, num_cols),\n",
    "        ])\n",
    "\n",
    "    if pipeline is None or fit:\n",
    "        pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "        fit_data = pipeline.fit_transform(data)\n",
    "    else:\n",
    "        fit_data = pipeline.transform(data)\n",
    "\n",
    "    feature_names = num_cols\n",
    "    return pd.DataFrame(fit_data, columns = feature_names)\n",
    "\n",
    "def map_string_to_int(data):\n",
    "    mapping = {\"yes\": 1, \"no\": 0, \"\": 0}\n",
    "    # For the following fields the values 0,1 are represented as yes,no. odd!\n",
    "    # edjefe,years of education of male head of household\n",
    "    # edjefa,years of education of female head of household\n",
    "    # dependency, Dependency rate\n",
    "    for col in [\"edjefe\", \"edjefa\", 'dependency']:\n",
    "        data[col] = data[col].apply(lambda x: mapping[x] if x in mapping else x).astype(float)\n",
    "    return data\n",
    "\n",
    "def remove_columns(data):\n",
    "    AGE_SQUARE = 'SQBage'\n",
    "    HOUSEHOlD_SIZE = 'tamhog'\n",
    "    cols_to_remove = [ ID_COLUMN, TARGET_COLUMN, HOUSE_HOLD_ID_COLUMN, AGE_SQUARE, HOUSEHOlD_SIZE ]\n",
    "    for col in cols_to_remove:\n",
    "        if col in data.columns:\n",
    "            data = data.drop(columns = col)\n",
    "    return data\n",
    "\n",
    "def prepare(csv_path = None, data = None):\n",
    "    if csv_path is None and data is None:\n",
    "        raise ValueError(\"Either csv_path or data must be provided\")\n",
    "    if csv_path is not None and data is not None:\n",
    "        raise ValueError(\"Only one of csv_path or data must be provided\")\n",
    "    raw_data = pd.read_csv(csv_path) if csv_path is not None else deepcopy(data)\n",
    "    raw_data = remove_columns(raw_data)\n",
    "    raw_data = map_string_to_int(raw_data)\n",
    "    return fill_and_encode(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "\n",
    "def prepare_model(\n",
    "    model, \n",
    "    train_x, \n",
    "    train_y, \n",
    "    param_grid = None, \n",
    "    boosting = 'not-xgb',\n",
    "    encoder = None,\n",
    "    test_size=DEFAULT_VALIDATION_SIZE, \n",
    "    cv = DEFAULT_CROSS_VALIDATION):\n",
    "    if boosting == 'xgb' and encoder is None:\n",
    "        raise ValueError(\"encoder must be provided for xgb boosting\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train_x, train_y, test_size=test_size, random_state=DEFAULT_RANDOM_STATE, stratify=train_y)\n",
    "\n",
    "    if param_grid is not None:\n",
    "        model = GridSearchCV(model, param_grid, cv=cv, scoring=\"f1_macro\", n_jobs=-1)\n",
    "\n",
    "    model.fit(X_train, y_train if boosting != 'xgb' else encoder.fit_transform(y_train))\n",
    "    \n",
    "    predictions = model.predict(X_test)\n",
    "    predictions = predictions if boosting != 'xgb' else encoder.inverse_transform(predictions)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "def oversample(X, y):\n",
    "    ros = RandomOverSampler(random_state=0)\n",
    "    return ros.fit_resample(X, y)\n",
    "\n",
    "def undersample(X, y):\n",
    "    ros = RandomUnderSampler(random_state=0)\n",
    "    return ros.fit_resample(X, y)\n",
    "\n",
    "def smotsample(X, y):\n",
    "    ros = SMOTETomek(sampling_strategy='auto')\n",
    "    return ros.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit(model, xgb_encoder = None):\n",
    "    test_data = pd.read_csv(TEST_CSV)\n",
    "    pred_input = prepare(TEST_CSV)\n",
    "    predictions = model.predict(pred_input)\n",
    "    predictions = predictions if xgb_encoder is None else xgb_encoder.inverse_transform(predictions)\n",
    "    submission_df = pd.DataFrame({ID_COLUMN: test_data[ID_COLUMN], TARGET_COLUMN: predictions})\n",
    "    submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = pd.read_csv(TRAIN_CSV)\n",
    "TARGET = csv_data[TARGET_COLUMN]\n",
    "DATA = prepare(data=csv_data)\n",
    "TRAINING_FEATURES = DATA.columns\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(DATA, TARGET, test_size=DEFAULT_TEST_SIZE, random_state=DEFAULT_RANDOM_STATE)\n",
    "oversampled_X, oversampled_y = oversample(X_train, y_train)\n",
    "smotsampled_X, smotsampled_y = smotsample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "x_params = {\n",
    "    'objective': ['multi:logistic'], \n",
    "    'num_class': [4], \n",
    "    'n_estimators': [100], \n",
    "    'max_depth': [10], \n",
    "    'learning_rate': [0.1], \n",
    "    'eval_metric': ['merror']\n",
    "}\n",
    "\n",
    "x_params = {\n",
    "    'eval_metric': ['merror'], \n",
    "    'learning_rate': [0.06], \n",
    "    'max_depth': [20], \n",
    "    'n_estimators': [300], \n",
    "    'num_class': [4], \n",
    "    'objective': ['multi:softprob']\n",
    "}\n",
    "\n",
    "x_params = {\n",
    " 'eval_metric': ['merror'],\n",
    " 'learning_rate': [1],\n",
    " 'max_depth': [10],\n",
    " 'n_estimators': [100],\n",
    " 'num_class': [4],\n",
    " 'objective': ['multi:softprob']\n",
    "}\n",
    "\n",
    "xg_boost = prepare_model(\n",
    "    XGBClassifier(), \n",
    "    smotsampled_X, \n",
    "    smotsampled_y, \n",
    "    param_grid=x_params, \n",
    "    boosting='xgb', \n",
    "    encoder=le,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit(xg_boost, le)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
